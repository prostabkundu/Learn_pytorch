{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWGAaewu098X0EUUhAHKWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prostabkundu/Learn_pytorch/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7ZIERyR-7y-q"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "data = fetch_california_housing()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(data.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmiHDJvj796z",
        "outputId": "ba93df69-667c-42d8-9248-b6d5c0418d2a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGCJ-8vs8Qk3",
        "outputId": "1f5092ac-6b09-4470-e69e-b9a5f5660b0d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1J7sMqv8T7_",
        "outputId": "8d162ed6-71ed-4ad7-e816-6b4c9f8a94f7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81LzNmXx8BXE",
        "outputId": "7d2955f4-d6d9-493e-a32e-8d6c3e8e1a05"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
              "          37.88      , -122.23      ],\n",
              "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
              "          37.86      , -122.22      ],\n",
              "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
              "          37.85      , -122.24      ],\n",
              "       ...,\n",
              "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
              "          39.43      , -121.22      ],\n",
              "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
              "          39.43      , -121.32      ],\n",
              "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
              "          39.37      , -121.24      ]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0Lkpl0Nn8TZJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQpfMkdZ8itm",
        "outputId": "9ea9d370-fe7a-4924-92a4-9500ecca2a97"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change numpy to tensor\n",
        "import torch\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()"
      ],
      "metadata": {
        "id": "Djw_EZ6Z8lG9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT1aae5e8rSG",
        "outputId": "a2c9ab4b-ffce-4e9b-a8af-ac884244d953"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16512, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGdjXiL_9AbH",
        "outputId": "329b64c9-9b6c-483b-a3b0-81ab6668221a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16512])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i92-y0on-hd8",
        "outputId": "d99bc1e5-4987-40db-9619-d34950e89ea8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4128])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define a ANN\n",
        "import torch.nn as nn\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "cC8xqgY08xVu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.unsqueeze(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R14iShp09qfn",
        "outputId": "8fecd907-c061-4de4-e74b-cb6ace57ccf7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0300],\n",
              "        [3.8210],\n",
              "        [1.7260],\n",
              "        ...,\n",
              "        [2.2210],\n",
              "        [2.8350],\n",
              "        [3.2500]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make a object of that ANN\n",
        "model = ANN()\n",
        "#train the model\n",
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "model.train()\n",
        "for epoch in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train.unsqueeze(1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye_-dY9z9Mdf",
        "outputId": "e585d913-75fd-4838-bd10-884e2f65be91"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 870.7487182617188\n",
            "Epoch: 1, Loss: 253.8228302001953\n",
            "Epoch: 2, Loss: 12.10602855682373\n",
            "Epoch: 3, Loss: 67.83248138427734\n",
            "Epoch: 4, Loss: 193.19471740722656\n",
            "Epoch: 5, Loss: 242.7002716064453\n",
            "Epoch: 6, Loss: 201.32736206054688\n",
            "Epoch: 7, Loss: 119.41913604736328\n",
            "Epoch: 8, Loss: 50.53347396850586\n",
            "Epoch: 9, Loss: 11.494767189025879\n",
            "Epoch: 10, Loss: 8.190342903137207\n",
            "Epoch: 11, Loss: 31.104793548583984\n",
            "Epoch: 12, Loss: 60.9791259765625\n",
            "Epoch: 13, Loss: 79.85782623291016\n",
            "Epoch: 14, Loss: 79.35310363769531\n",
            "Epoch: 15, Loss: 61.9973258972168\n",
            "Epoch: 16, Loss: 37.26601791381836\n",
            "Epoch: 17, Loss: 15.97340202331543\n",
            "Epoch: 18, Loss: 5.5885796546936035\n",
            "Epoch: 19, Loss: 7.675903797149658\n",
            "Epoch: 20, Loss: 18.05124282836914\n",
            "Epoch: 21, Loss: 29.479291915893555\n",
            "Epoch: 22, Loss: 35.534080505371094\n",
            "Epoch: 23, Loss: 33.53498840332031\n",
            "Epoch: 24, Loss: 25.13162612915039\n",
            "Epoch: 25, Loss: 14.728659629821777\n",
            "Epoch: 26, Loss: 6.9547247886657715\n",
            "Epoch: 27, Loss: 4.47029972076416\n",
            "Epoch: 28, Loss: 7.033911228179932\n",
            "Epoch: 29, Loss: 12.062213897705078\n",
            "Epoch: 30, Loss: 16.241580963134766\n",
            "Epoch: 31, Loss: 17.267431259155273\n",
            "Epoch: 32, Loss: 14.811691284179688\n",
            "Epoch: 33, Loss: 10.36040210723877\n",
            "Epoch: 34, Loss: 6.178747653961182\n",
            "Epoch: 35, Loss: 4.05998420715332\n",
            "Epoch: 36, Loss: 4.48573637008667\n",
            "Epoch: 37, Loss: 6.5635271072387695\n",
            "Epoch: 38, Loss: 8.692115783691406\n",
            "Epoch: 39, Loss: 9.517439842224121\n",
            "Epoch: 40, Loss: 8.621382713317871\n",
            "Epoch: 41, Loss: 6.600614070892334\n",
            "Epoch: 42, Loss: 4.581940174102783\n",
            "Epoch: 43, Loss: 3.546783208847046\n",
            "Epoch: 44, Loss: 3.6823627948760986\n",
            "Epoch: 45, Loss: 4.60205602645874\n",
            "Epoch: 46, Loss: 5.508798599243164\n",
            "Epoch: 47, Loss: 5.759247779846191\n",
            "Epoch: 48, Loss: 5.208335876464844\n",
            "Epoch: 49, Loss: 4.209524154663086\n",
            "Epoch: 50, Loss: 3.3324973583221436\n",
            "Epoch: 51, Loss: 2.9932029247283936\n",
            "Epoch: 52, Loss: 3.2299294471740723\n",
            "Epoch: 53, Loss: 3.7301056385040283\n",
            "Epoch: 54, Loss: 4.070241451263428\n",
            "Epoch: 55, Loss: 3.995408535003662\n",
            "Epoch: 56, Loss: 3.559194564819336\n",
            "Epoch: 57, Loss: 3.041325092315674\n",
            "Epoch: 58, Loss: 2.7319765090942383\n",
            "Epoch: 59, Loss: 2.7379298210144043\n",
            "Epoch: 60, Loss: 2.944509744644165\n",
            "Epoch: 61, Loss: 3.1262052059173584\n",
            "Epoch: 62, Loss: 3.1133241653442383\n",
            "Epoch: 63, Loss: 2.893195867538452\n",
            "Epoch: 64, Loss: 2.5921213626861572\n",
            "Epoch: 65, Loss: 2.3691446781158447\n",
            "Epoch: 66, Loss: 2.3029658794403076\n",
            "Epoch: 67, Loss: 2.349696636199951\n",
            "Epoch: 68, Loss: 2.394362449645996\n",
            "Epoch: 69, Loss: 2.345064401626587\n",
            "Epoch: 70, Loss: 2.2149269580841064\n",
            "Epoch: 71, Loss: 2.1459758281707764\n",
            "Epoch: 72, Loss: 2.027843475341797\n",
            "Epoch: 73, Loss: 1.879418969154358\n",
            "Epoch: 74, Loss: 1.7866172790527344\n",
            "Epoch: 75, Loss: 1.7332531213760376\n",
            "Epoch: 76, Loss: 1.7016688585281372\n",
            "Epoch: 77, Loss: 1.6733444929122925\n",
            "Epoch: 78, Loss: 1.630547285079956\n",
            "Epoch: 79, Loss: 1.563862919807434\n",
            "Epoch: 80, Loss: 1.490620732307434\n",
            "Epoch: 81, Loss: 1.4741203784942627\n",
            "Epoch: 82, Loss: 1.4887757301330566\n",
            "Epoch: 83, Loss: 1.4461160898208618\n",
            "Epoch: 84, Loss: 1.4586883783340454\n",
            "Epoch: 85, Loss: 1.4348068237304688\n",
            "Epoch: 86, Loss: 1.3814349174499512\n",
            "Epoch: 87, Loss: 1.3763720989227295\n",
            "Epoch: 88, Loss: 1.3902339935302734\n",
            "Epoch: 89, Loss: 1.3674771785736084\n",
            "Epoch: 90, Loss: 1.3431657552719116\n",
            "Epoch: 91, Loss: 1.3512197732925415\n",
            "Epoch: 92, Loss: 1.3484699726104736\n",
            "Epoch: 93, Loss: 1.3172023296356201\n",
            "Epoch: 94, Loss: 1.308968186378479\n",
            "Epoch: 95, Loss: 1.3094674348831177\n",
            "Epoch: 96, Loss: 1.2866439819335938\n",
            "Epoch: 97, Loss: 1.2823286056518555\n",
            "Epoch: 98, Loss: 1.2905843257904053\n",
            "Epoch: 99, Loss: 1.2742244005203247\n",
            "Epoch: 100, Loss: 1.2643553018569946\n",
            "Epoch: 101, Loss: 1.2557097673416138\n",
            "Epoch: 102, Loss: 1.242506504058838\n",
            "Epoch: 103, Loss: 1.24347984790802\n",
            "Epoch: 104, Loss: 1.2352933883666992\n",
            "Epoch: 105, Loss: 1.2205199003219604\n",
            "Epoch: 106, Loss: 1.2118107080459595\n",
            "Epoch: 107, Loss: 1.1996650695800781\n",
            "Epoch: 108, Loss: 1.2000635862350464\n",
            "Epoch: 109, Loss: 1.1936451196670532\n",
            "Epoch: 110, Loss: 1.1872973442077637\n",
            "Epoch: 111, Loss: 1.180281400680542\n",
            "Epoch: 112, Loss: 1.1702364683151245\n",
            "Epoch: 113, Loss: 1.1686900854110718\n",
            "Epoch: 114, Loss: 1.166584849357605\n",
            "Epoch: 115, Loss: 1.1610076427459717\n",
            "Epoch: 116, Loss: 1.1597940921783447\n",
            "Epoch: 117, Loss: 1.1561214923858643\n",
            "Epoch: 118, Loss: 1.147908091545105\n",
            "Epoch: 119, Loss: 1.1428158283233643\n",
            "Epoch: 120, Loss: 1.1400800943374634\n",
            "Epoch: 121, Loss: 1.1400433778762817\n",
            "Epoch: 122, Loss: 1.146865963935852\n",
            "Epoch: 123, Loss: 1.1648216247558594\n",
            "Epoch: 124, Loss: 1.1768568754196167\n",
            "Epoch: 125, Loss: 1.1388778686523438\n",
            "Epoch: 126, Loss: 1.1284866333007812\n",
            "Epoch: 127, Loss: 1.1519838571548462\n",
            "Epoch: 128, Loss: 1.1299654245376587\n",
            "Epoch: 129, Loss: 1.1202517747879028\n",
            "Epoch: 130, Loss: 1.1365251541137695\n",
            "Epoch: 131, Loss: 1.1165728569030762\n",
            "Epoch: 132, Loss: 1.1162936687469482\n",
            "Epoch: 133, Loss: 1.12310791015625\n",
            "Epoch: 134, Loss: 1.1057360172271729\n",
            "Epoch: 135, Loss: 1.111895203590393\n",
            "Epoch: 136, Loss: 1.109712839126587\n",
            "Epoch: 137, Loss: 1.0985018014907837\n",
            "Epoch: 138, Loss: 1.1055399179458618\n",
            "Epoch: 139, Loss: 1.098183512687683\n",
            "Epoch: 140, Loss: 1.0931859016418457\n",
            "Epoch: 141, Loss: 1.097425937652588\n",
            "Epoch: 142, Loss: 1.0888473987579346\n",
            "Epoch: 143, Loss: 1.0876280069351196\n",
            "Epoch: 144, Loss: 1.0886262655258179\n",
            "Epoch: 145, Loss: 1.0811494588851929\n",
            "Epoch: 146, Loss: 1.0813486576080322\n",
            "Epoch: 147, Loss: 1.0802932977676392\n",
            "Epoch: 148, Loss: 1.074294090270996\n",
            "Epoch: 149, Loss: 1.0744552612304688\n",
            "Epoch: 150, Loss: 1.0725646018981934\n",
            "Epoch: 151, Loss: 1.0676071643829346\n",
            "Epoch: 152, Loss: 1.0672023296356201\n",
            "Epoch: 153, Loss: 1.0653541088104248\n",
            "Epoch: 154, Loss: 1.06100332736969\n",
            "Epoch: 155, Loss: 1.0598289966583252\n",
            "Epoch: 156, Loss: 1.0583714246749878\n",
            "Epoch: 157, Loss: 1.0545103549957275\n",
            "Epoch: 158, Loss: 1.0523960590362549\n",
            "Epoch: 159, Loss: 1.0512737035751343\n",
            "Epoch: 160, Loss: 1.048229694366455\n",
            "Epoch: 161, Loss: 1.0452210903167725\n",
            "Epoch: 162, Loss: 1.0437655448913574\n",
            "Epoch: 163, Loss: 1.0418306589126587\n",
            "Epoch: 164, Loss: 1.0387612581253052\n",
            "Epoch: 165, Loss: 1.0362627506256104\n",
            "Epoch: 166, Loss: 1.034617304801941\n",
            "Epoch: 167, Loss: 1.03249990940094\n",
            "Epoch: 168, Loss: 1.0296880006790161\n",
            "Epoch: 169, Loss: 1.0271168947219849\n",
            "Epoch: 170, Loss: 1.0251365900039673\n",
            "Epoch: 171, Loss: 1.0231983661651611\n",
            "Epoch: 172, Loss: 1.0208141803741455\n",
            "Epoch: 173, Loss: 1.018179178237915\n",
            "Epoch: 174, Loss: 1.015701413154602\n",
            "Epoch: 175, Loss: 1.01352059841156\n",
            "Epoch: 176, Loss: 1.0114684104919434\n",
            "Epoch: 177, Loss: 1.0093334913253784\n",
            "Epoch: 178, Loss: 1.0070472955703735\n",
            "Epoch: 179, Loss: 1.004633903503418\n",
            "Epoch: 180, Loss: 1.0021811723709106\n",
            "Epoch: 181, Loss: 0.9997496008872986\n",
            "Epoch: 182, Loss: 0.9973726272583008\n",
            "Epoch: 183, Loss: 0.9950883388519287\n",
            "Epoch: 184, Loss: 0.9929103255271912\n",
            "Epoch: 185, Loss: 0.990880012512207\n",
            "Epoch: 186, Loss: 0.9894702434539795\n",
            "Epoch: 187, Loss: 0.9892364144325256\n",
            "Epoch: 188, Loss: 0.9920597672462463\n",
            "Epoch: 189, Loss: 0.9949054718017578\n",
            "Epoch: 190, Loss: 1.002140760421753\n",
            "Epoch: 191, Loss: 0.9908098578453064\n",
            "Epoch: 192, Loss: 0.9806833863258362\n",
            "Epoch: 193, Loss: 0.973273515701294\n",
            "Epoch: 194, Loss: 0.9766480922698975\n",
            "Epoch: 195, Loss: 0.9783388376235962\n",
            "Epoch: 196, Loss: 0.9689223170280457\n",
            "Epoch: 197, Loss: 0.9659680724143982\n",
            "Epoch: 198, Loss: 0.9689064025878906\n",
            "Epoch: 199, Loss: 0.9641512632369995\n",
            "Epoch: 200, Loss: 0.9578883647918701\n",
            "Epoch: 201, Loss: 0.9568697214126587\n",
            "Epoch: 202, Loss: 0.9577723145484924\n",
            "Epoch: 203, Loss: 0.9557867646217346\n",
            "Epoch: 204, Loss: 0.9503679871559143\n",
            "Epoch: 205, Loss: 0.9466800689697266\n",
            "Epoch: 206, Loss: 0.9457775950431824\n",
            "Epoch: 207, Loss: 0.9449528455734253\n",
            "Epoch: 208, Loss: 0.9421721696853638\n",
            "Epoch: 209, Loss: 0.938047468662262\n",
            "Epoch: 210, Loss: 0.9353342652320862\n",
            "Epoch: 211, Loss: 0.9340429306030273\n",
            "Epoch: 212, Loss: 0.9323816895484924\n",
            "Epoch: 213, Loss: 0.9276354908943176\n",
            "Epoch: 214, Loss: 1.16436767578125\n",
            "Epoch: 215, Loss: 3.7733395099639893\n",
            "Epoch: 216, Loss: 1.0183172225952148\n",
            "Epoch: 217, Loss: 2.7042412757873535\n",
            "Epoch: 218, Loss: 1.938319444656372\n",
            "Epoch: 219, Loss: 1.0472491979599\n",
            "Epoch: 220, Loss: 1.9458415508270264\n",
            "Epoch: 221, Loss: 2.104625940322876\n",
            "Epoch: 222, Loss: 1.170183539390564\n",
            "Epoch: 223, Loss: 1.1834543943405151\n",
            "Epoch: 224, Loss: 1.7727975845336914\n",
            "Epoch: 225, Loss: 1.4875195026397705\n",
            "Epoch: 226, Loss: 0.9742607474327087\n",
            "Epoch: 227, Loss: 1.2167837619781494\n",
            "Epoch: 228, Loss: 1.5014151334762573\n",
            "Epoch: 229, Loss: 1.1829990148544312\n",
            "Epoch: 230, Loss: 0.9645306468009949\n",
            "Epoch: 231, Loss: 1.2116661071777344\n",
            "Epoch: 232, Loss: 1.303985834121704\n",
            "Epoch: 233, Loss: 1.051650881767273\n",
            "Epoch: 234, Loss: 0.9759001731872559\n",
            "Epoch: 235, Loss: 1.1552485227584839\n",
            "Epoch: 236, Loss: 1.155744194984436\n",
            "Epoch: 237, Loss: 0.9793464541435242\n",
            "Epoch: 238, Loss: 0.9745780229568481\n",
            "Epoch: 239, Loss: 1.092290997505188\n",
            "Epoch: 240, Loss: 1.059285283088684\n",
            "Epoch: 241, Loss: 0.9502131342887878\n",
            "Epoch: 242, Loss: 0.9756847620010376\n",
            "Epoch: 243, Loss: 1.047741413116455\n",
            "Epoch: 244, Loss: 1.0053859949111938\n",
            "Epoch: 245, Loss: 0.9412416815757751\n",
            "Epoch: 246, Loss: 0.9730793833732605\n",
            "Epoch: 247, Loss: 1.0096807479858398\n",
            "Epoch: 248, Loss: 0.9678622484207153\n",
            "Epoch: 249, Loss: 0.9335376620292664\n",
            "Epoch: 250, Loss: 0.9628951549530029\n",
            "Epoch: 251, Loss: 0.9776915311813354\n",
            "Epoch: 252, Loss: 0.9437479376792908\n",
            "Epoch: 253, Loss: 0.9301157593727112\n",
            "Epoch: 254, Loss: 0.9536768794059753\n",
            "Epoch: 255, Loss: 0.9551353454589844\n",
            "Epoch: 256, Loss: 0.9301403164863586\n",
            "Epoch: 257, Loss: 0.9276795387268066\n",
            "Epoch: 258, Loss: 0.9427748322486877\n",
            "Epoch: 259, Loss: 0.9360702633857727\n",
            "Epoch: 260, Loss: 0.9189603328704834\n",
            "Epoch: 261, Loss: 0.9216843843460083\n",
            "Epoch: 262, Loss: 0.929142415523529\n",
            "Epoch: 263, Loss: 0.9196919202804565\n",
            "Epoch: 264, Loss: 0.9101099967956543\n",
            "Epoch: 265, Loss: 0.9145424962043762\n",
            "Epoch: 266, Loss: 0.916174054145813\n",
            "Epoch: 267, Loss: 0.9067661762237549\n",
            "Epoch: 268, Loss: 0.9020583033561707\n",
            "Epoch: 269, Loss: 0.9048792719841003\n",
            "Epoch: 270, Loss: 0.9020597338676453\n",
            "Epoch: 271, Loss: 0.8948354125022888\n",
            "Epoch: 272, Loss: 0.8937245011329651\n",
            "Epoch: 273, Loss: 0.8940449357032776\n",
            "Epoch: 274, Loss: 0.8885352611541748\n",
            "Epoch: 275, Loss: 0.8839939832687378\n",
            "Epoch: 276, Loss: 0.8836457133293152\n",
            "Epoch: 277, Loss: 0.8798304200172424\n",
            "Epoch: 278, Loss: 0.8734278082847595\n",
            "Epoch: 279, Loss: 0.8713892102241516\n",
            "Epoch: 280, Loss: 0.8674741387367249\n",
            "Epoch: 281, Loss: 0.8614548444747925\n",
            "Epoch: 282, Loss: 0.8590912222862244\n",
            "Epoch: 283, Loss: 0.854035496711731\n",
            "Epoch: 284, Loss: 0.847672164440155\n",
            "Epoch: 285, Loss: 0.8453936576843262\n",
            "Epoch: 286, Loss: 0.8379143476486206\n",
            "Epoch: 287, Loss: 0.8343787789344788\n",
            "Epoch: 288, Loss: 0.828037679195404\n",
            "Epoch: 289, Loss: 0.8229851126670837\n",
            "Epoch: 290, Loss: 0.8173779845237732\n",
            "Epoch: 291, Loss: 0.812082052230835\n",
            "Epoch: 292, Loss: 0.8054402470588684\n",
            "Epoch: 293, Loss: 0.800929605960846\n",
            "Epoch: 294, Loss: 0.7928584814071655\n",
            "Epoch: 295, Loss: 0.7886517643928528\n",
            "Epoch: 296, Loss: 0.7839204668998718\n",
            "Epoch: 297, Loss: 0.7770069241523743\n",
            "Epoch: 298, Loss: 0.7726898789405823\n",
            "Epoch: 299, Loss: 0.7698535323143005\n",
            "Epoch: 300, Loss: 0.7655317783355713\n",
            "Epoch: 301, Loss: 0.7590920329093933\n",
            "Epoch: 302, Loss: 0.7528687715530396\n",
            "Epoch: 303, Loss: 0.748725950717926\n",
            "Epoch: 304, Loss: 0.7467353940010071\n",
            "Epoch: 305, Loss: 0.7469097971916199\n",
            "Epoch: 306, Loss: 0.7479206919670105\n",
            "Epoch: 307, Loss: 0.7420610189437866\n",
            "Epoch: 308, Loss: 0.7315345406532288\n",
            "Epoch: 309, Loss: 0.7304678559303284\n",
            "Epoch: 310, Loss: 0.7332323789596558\n",
            "Epoch: 311, Loss: 0.7272632122039795\n",
            "Epoch: 312, Loss: 0.7193430066108704\n",
            "Epoch: 313, Loss: 0.7199551463127136\n",
            "Epoch: 314, Loss: 0.7206632494926453\n",
            "Epoch: 315, Loss: 0.7142306566238403\n",
            "Epoch: 316, Loss: 0.7087528705596924\n",
            "Epoch: 317, Loss: 0.7095268368721008\n",
            "Epoch: 318, Loss: 0.7090331315994263\n",
            "Epoch: 319, Loss: 0.7032573223114014\n",
            "Epoch: 320, Loss: 0.698964536190033\n",
            "Epoch: 321, Loss: 0.6991689205169678\n",
            "Epoch: 322, Loss: 0.6988550424575806\n",
            "Epoch: 323, Loss: 0.6949505805969238\n",
            "Epoch: 324, Loss: 0.6902126669883728\n",
            "Epoch: 325, Loss: 0.6883593201637268\n",
            "Epoch: 326, Loss: 0.6884880661964417\n",
            "Epoch: 327, Loss: 0.687572181224823\n",
            "Epoch: 328, Loss: 0.6845420002937317\n",
            "Epoch: 329, Loss: 0.6805934309959412\n",
            "Epoch: 330, Loss: 0.677736222743988\n",
            "Epoch: 331, Loss: 0.6765225529670715\n",
            "Epoch: 332, Loss: 0.6761228442192078\n",
            "Epoch: 333, Loss: 0.6756933927536011\n",
            "Epoch: 334, Loss: 0.6745840311050415\n",
            "Epoch: 335, Loss: 0.6726507544517517\n",
            "Epoch: 336, Loss: 0.6697688102722168\n",
            "Epoch: 337, Loss: 0.6666814088821411\n",
            "Epoch: 338, Loss: 0.6639612913131714\n",
            "Epoch: 339, Loss: 0.6619083285331726\n",
            "Epoch: 340, Loss: 0.6604658961296082\n",
            "Epoch: 341, Loss: 0.659477710723877\n",
            "Epoch: 342, Loss: 0.6590479612350464\n",
            "Epoch: 343, Loss: 0.6597521901130676\n",
            "Epoch: 344, Loss: 0.6626167893409729\n",
            "Epoch: 345, Loss: 0.6673627495765686\n",
            "Epoch: 346, Loss: 0.6705605983734131\n",
            "Epoch: 347, Loss: 0.6638533473014832\n",
            "Epoch: 348, Loss: 0.652027428150177\n",
            "Epoch: 349, Loss: 0.6476694345474243\n",
            "Epoch: 350, Loss: 0.6530476808547974\n",
            "Epoch: 351, Loss: 0.6568835377693176\n",
            "Epoch: 352, Loss: 0.6502364873886108\n",
            "Epoch: 353, Loss: 0.6430057883262634\n",
            "Epoch: 354, Loss: 0.6439275145530701\n",
            "Epoch: 355, Loss: 0.6476914286613464\n",
            "Epoch: 356, Loss: 0.6458368897438049\n",
            "Epoch: 357, Loss: 0.6396488547325134\n",
            "Epoch: 358, Loss: 0.6380115747451782\n",
            "Epoch: 359, Loss: 0.6406263709068298\n",
            "Epoch: 360, Loss: 0.6405063271522522\n",
            "Epoch: 361, Loss: 0.6367028951644897\n",
            "Epoch: 362, Loss: 0.6335316896438599\n",
            "Epoch: 363, Loss: 0.633870542049408\n",
            "Epoch: 364, Loss: 0.6351832747459412\n",
            "Epoch: 365, Loss: 0.6339663863182068\n",
            "Epoch: 366, Loss: 0.6310360431671143\n",
            "Epoch: 367, Loss: 0.6290141344070435\n",
            "Epoch: 368, Loss: 0.6290287971496582\n",
            "Epoch: 369, Loss: 0.6297430396080017\n",
            "Epoch: 370, Loss: 0.629188597202301\n",
            "Epoch: 371, Loss: 0.627302885055542\n",
            "Epoch: 372, Loss: 0.6252126693725586\n",
            "Epoch: 373, Loss: 0.6240847706794739\n",
            "Epoch: 374, Loss: 0.6239060163497925\n",
            "Epoch: 375, Loss: 0.6239308714866638\n",
            "Epoch: 376, Loss: 0.6235373020172119\n",
            "Epoch: 377, Loss: 0.6224057078361511\n",
            "Epoch: 378, Loss: 0.6210355758666992\n",
            "Epoch: 379, Loss: 0.619728147983551\n",
            "Epoch: 380, Loss: 0.618801474571228\n",
            "Epoch: 381, Loss: 0.6182535886764526\n",
            "Epoch: 382, Loss: 0.6179226040840149\n",
            "Epoch: 383, Loss: 0.6176397204399109\n",
            "Epoch: 384, Loss: 0.6173195242881775\n",
            "Epoch: 385, Loss: 0.6170255541801453\n",
            "Epoch: 386, Loss: 0.6166994571685791\n",
            "Epoch: 387, Loss: 0.6164358258247375\n",
            "Epoch: 388, Loss: 0.6161621809005737\n",
            "Epoch: 389, Loss: 0.6159409880638123\n",
            "Epoch: 390, Loss: 0.6157167553901672\n",
            "Epoch: 391, Loss: 0.6155492663383484\n",
            "Epoch: 392, Loss: 0.6152550578117371\n",
            "Epoch: 393, Loss: 0.6149389743804932\n",
            "Epoch: 394, Loss: 0.6141232252120972\n",
            "Epoch: 395, Loss: 0.6130960583686829\n",
            "Epoch: 396, Loss: 0.6115699410438538\n",
            "Epoch: 397, Loss: 0.6100631952285767\n",
            "Epoch: 398, Loss: 0.6086114645004272\n",
            "Epoch: 399, Loss: 0.6074524521827698\n",
            "Epoch: 400, Loss: 0.6066675782203674\n",
            "Epoch: 401, Loss: 0.60621178150177\n",
            "Epoch: 402, Loss: 0.6060149073600769\n",
            "Epoch: 403, Loss: 0.606023907661438\n",
            "Epoch: 404, Loss: 0.6062965989112854\n",
            "Epoch: 405, Loss: 0.6068530082702637\n",
            "Epoch: 406, Loss: 0.6078551411628723\n",
            "Epoch: 407, Loss: 0.6092479825019836\n",
            "Epoch: 408, Loss: 0.6112064123153687\n",
            "Epoch: 409, Loss: 0.612536609172821\n",
            "Epoch: 410, Loss: 0.613150417804718\n",
            "Epoch: 411, Loss: 0.6103604435920715\n",
            "Epoch: 412, Loss: 0.6062986254692078\n",
            "Epoch: 413, Loss: 0.6020119190216064\n",
            "Epoch: 414, Loss: 0.6002196669578552\n",
            "Epoch: 415, Loss: 0.6011191606521606\n",
            "Epoch: 416, Loss: 0.602982223033905\n",
            "Epoch: 417, Loss: 0.6040382981300354\n",
            "Epoch: 418, Loss: 0.6028913259506226\n",
            "Epoch: 419, Loss: 0.60064697265625\n",
            "Epoch: 420, Loss: 0.5983921885490417\n",
            "Epoch: 421, Loss: 0.5974283814430237\n",
            "Epoch: 422, Loss: 0.5977417230606079\n",
            "Epoch: 423, Loss: 0.5986303091049194\n",
            "Epoch: 424, Loss: 0.5994163155555725\n",
            "Epoch: 425, Loss: 0.5994395017623901\n",
            "Epoch: 426, Loss: 0.5987749695777893\n",
            "Epoch: 427, Loss: 0.5974783897399902\n",
            "Epoch: 428, Loss: 0.5960798263549805\n",
            "Epoch: 429, Loss: 0.5949015021324158\n",
            "Epoch: 430, Loss: 0.5941829085350037\n",
            "Epoch: 431, Loss: 0.5939258337020874\n",
            "Epoch: 432, Loss: 0.5939828157424927\n",
            "Epoch: 433, Loss: 0.5942021012306213\n",
            "Epoch: 434, Loss: 0.5944276452064514\n",
            "Epoch: 435, Loss: 0.594550371170044\n",
            "Epoch: 436, Loss: 0.5944578647613525\n",
            "Epoch: 437, Loss: 0.5942072868347168\n",
            "Epoch: 438, Loss: 0.5937036275863647\n",
            "Epoch: 439, Loss: 0.593100905418396\n",
            "Epoch: 440, Loss: 0.5923685431480408\n",
            "Epoch: 441, Loss: 0.5916741490364075\n",
            "Epoch: 442, Loss: 0.5910112857818604\n",
            "Epoch: 443, Loss: 0.5904320478439331\n",
            "Epoch: 444, Loss: 0.5899307727813721\n",
            "Epoch: 445, Loss: 0.5895050764083862\n",
            "Epoch: 446, Loss: 0.5891239047050476\n",
            "Epoch: 447, Loss: 0.5887744426727295\n",
            "Epoch: 448, Loss: 0.5884573459625244\n",
            "Epoch: 449, Loss: 0.5881500244140625\n",
            "Epoch: 450, Loss: 0.5878520011901855\n",
            "Epoch: 451, Loss: 0.5875640511512756\n",
            "Epoch: 452, Loss: 0.5872892737388611\n",
            "Epoch: 453, Loss: 0.587032675743103\n",
            "Epoch: 454, Loss: 0.5868167281150818\n",
            "Epoch: 455, Loss: 0.5866608619689941\n",
            "Epoch: 456, Loss: 0.5866470336914062\n",
            "Epoch: 457, Loss: 0.5869141221046448\n",
            "Epoch: 458, Loss: 0.5877315998077393\n",
            "Epoch: 459, Loss: 0.5897494554519653\n",
            "Epoch: 460, Loss: 0.5936264395713806\n",
            "Epoch: 461, Loss: 0.6011300683021545\n",
            "Epoch: 462, Loss: 0.6100807189941406\n",
            "Epoch: 463, Loss: 0.6197996735572815\n",
            "Epoch: 464, Loss: 0.6127331256866455\n",
            "Epoch: 465, Loss: 0.5977293252944946\n",
            "Epoch: 466, Loss: 0.5843943357467651\n",
            "Epoch: 467, Loss: 0.5884286165237427\n",
            "Epoch: 468, Loss: 0.5991102457046509\n",
            "Epoch: 469, Loss: 0.5951066613197327\n",
            "Epoch: 470, Loss: 0.5851640701293945\n",
            "Epoch: 471, Loss: 0.5840640068054199\n",
            "Epoch: 472, Loss: 0.5906738042831421\n",
            "Epoch: 473, Loss: 0.5919298529624939\n",
            "Epoch: 474, Loss: 0.5846685171127319\n",
            "Epoch: 475, Loss: 0.5819775462150574\n",
            "Epoch: 476, Loss: 0.5861063599586487\n",
            "Epoch: 477, Loss: 0.5880600810050964\n",
            "Epoch: 478, Loss: 0.5848678350448608\n",
            "Epoch: 479, Loss: 0.580885648727417\n",
            "Epoch: 480, Loss: 0.5813939571380615\n",
            "Epoch: 481, Loss: 0.5843222141265869\n",
            "Epoch: 482, Loss: 0.584654688835144\n",
            "Epoch: 483, Loss: 0.582264244556427\n",
            "Epoch: 484, Loss: 0.5796915888786316\n",
            "Epoch: 485, Loss: 0.5795735120773315\n",
            "Epoch: 486, Loss: 0.5811776518821716\n",
            "Epoch: 487, Loss: 0.5819746851921082\n",
            "Epoch: 488, Loss: 0.5809134840965271\n",
            "Epoch: 489, Loss: 0.5789893865585327\n",
            "Epoch: 490, Loss: 0.5780386924743652\n",
            "Epoch: 491, Loss: 0.5785040855407715\n",
            "Epoch: 492, Loss: 0.5792935490608215\n",
            "Epoch: 493, Loss: 0.5792894959449768\n",
            "Epoch: 494, Loss: 0.5782076120376587\n",
            "Epoch: 495, Loss: 0.5770835280418396\n",
            "Epoch: 496, Loss: 0.5766009092330933\n",
            "Epoch: 497, Loss: 0.576804518699646\n",
            "Epoch: 498, Loss: 0.5771142840385437\n",
            "Epoch: 499, Loss: 0.5769217014312744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate with test data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(X_test)\n",
        "    loss = criterion(output, y_test.unsqueeze(1))\n",
        "    print('Test Loss: {}'.format(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VO2QPjl9c4v",
        "outputId": "92622757-8f23-4cff-f0a6-c06b5bac0069"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5940712094306946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IjEOp-S-KZO"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}